# AIåœ¨HRé¢†åŸŸçš„æ¯å‘¨åº”ç”¨ - ä½¿ç”¨è¯´æ˜

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªå±•ç¤ºAIåœ¨äººåŠ›èµ„æºå„é¢†åŸŸåº”ç”¨çš„ç½‘é¡µåº”ç”¨ï¼Œæ¯å‘¨æ›´æ–°æœ€æ–°çš„AI+HRèµ„è®¯ã€‚

**æ¶µç›–é¢†åŸŸï¼š**
- ğŸ’° è–ªé…¬ç¦åˆ©
- ğŸ“ äººæ‰å‘å±•
- ğŸ¢ ç»„ç»‡å‘å±•
- ğŸ¨ ä¼ä¸šæ–‡åŒ–
- âš™ï¸ SSCå…±äº«æœåŠ¡

## ğŸ“ é¡¹ç›®æ–‡ä»¶

```
ai_hr_weekly.html        # ä¸»é¡µé¢ï¼ˆç°ä»£å¡ç‰‡å¼å¸ƒå±€ï¼‰
hr_news_scraper.py       # Pythonçˆ¬è™«è„šæœ¬ï¼ˆè‡ªåŠ¨æŠ“å–æ–°é—»ï¼‰
hr_news_data.json        # æ–°é—»æ•°æ®æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰
README_AI_HR.md          # ä½¿ç”¨è¯´æ˜ï¼ˆæœ¬æ–‡ä»¶ï¼‰
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æœ¬åœ°æŸ¥çœ‹

ç›´æ¥ç”¨æµè§ˆå™¨æ‰“å¼€ `ai_hr_weekly.html` æ–‡ä»¶å³å¯æŸ¥çœ‹ã€‚

### 2. æ›´æ–°æ•°æ®

è¿è¡ŒPythonè„šæœ¬æ›´æ–°æ–°é—»æ•°æ®ï¼š

```bash
python hr_news_scraper.py
```

è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆ10æ¡AI+HRæ–°é—»ï¼‰
- æŒ‰å‘¨åˆ†ç»„å’ŒæŒ‰ç±»åˆ«åˆ†ç±»
- ä¿å­˜åˆ° `hr_news_data.json`

### 3. åˆ·æ–°é¡µé¢

æ›´æ–°æ•°æ®åï¼Œåˆ·æ–°æµè§ˆå™¨é¡µé¢å³å¯çœ‹åˆ°æœ€æ–°å†…å®¹ã€‚

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### æ‰‹åŠ¨ç¼–è¾‘æ•°æ®

æ‰“å¼€ `hr_news_data.json`ï¼ŒæŒ‰ä»¥ä¸‹æ ¼å¼æ·»åŠ æ–°é—»ï¼š

```json
{
  "title": "æ–°é—»æ ‡é¢˜",
  "description": "æ–°é—»æè¿°",
  "category": "è–ªé…¬ç¦åˆ©",  // æˆ–: äººæ‰å‘å±•ã€ç»„ç»‡å‘å±•ã€ä¼ä¸šæ–‡åŒ–ã€SSC
  "source": "æ¥æºç½‘ç«™",
  "link": "https://example.com/article",
  "week": "2026å¹´ç¬¬9å‘¨",
  "date": "2026å¹´02æœˆ23æ—¥"
}
```

### ä¿®æ”¹ç½‘é¡µæ ·å¼

ç¼–è¾‘ `ai_hr_weekly.html` ä¸­çš„ CSS æ ·å¼ï¼š
- ä¿®æ”¹é¢œè‰²ï¼šæŸ¥æ‰¾ `#667eea` å’Œ `#764ba2`ï¼ˆä¸»é¢˜è‰²ï¼‰
- ä¿®æ”¹å­—ä½“ï¼šæŸ¥æ‰¾ `font-family`
- ä¿®æ”¹å¸ƒå±€ï¼šæŸ¥æ‰¾ `.cards-grid` çš„ grid è®¾ç½®

## ğŸŒ å‘å¸ƒåˆ°GitHub Pages

### æ­¥éª¤1ï¼šåˆ›å»ºGitHubä»“åº“

```bash
# åˆå§‹åŒ–gitä»“åº“
git init

# æ·»åŠ æ–‡ä»¶
git add ai_hr_weekly.html hr_news_data.json hr_news_scraper.py README_AI_HR.md

# æäº¤
git commit -m "Initial commit: AI in HR Weekly Application"

# å…³è”è¿œç¨‹ä»“åº“ï¼ˆæ›¿æ¢ä¸ºä½ çš„ä»“åº“åœ°å€ï¼‰
git remote add origin https://github.com/ä½ çš„ç”¨æˆ·å/ai-hr-weekly.git

# æ¨é€åˆ°GitHub
git push -u origin master
```

### æ­¥éª¤2ï¼šå¯ç”¨GitHub Pages

1. è¿›å…¥GitHubä»“åº“é¡µé¢
2. ç‚¹å‡» **Settings** > **Pages**
3. Source é€‰æ‹© **main** æˆ– **master** åˆ†æ”¯
4. ä¿å­˜åç­‰å¾…å‡ åˆ†é’Ÿ

### æ­¥éª¤3ï¼šè®¿é—®ç½‘ç«™

ç½‘ç«™åœ°å€ï¼š`https://ä½ çš„ç”¨æˆ·å.github.io/ai-hr-weekly/ai_hr_weekly.html`

## ğŸ”„ æ¯å‘¨è‡ªåŠ¨æ›´æ–°ï¼ˆå¯é€‰ï¼‰

### æ–¹æ¡ˆ1ï¼šGitHub Actionsï¼ˆæ¨èï¼‰

åˆ›å»º `.github/workflows/update-news.yml`ï¼š

```yaml
name: Update HR News

on:
  schedule:
    # æ¯å‘¨ä¸€æ—©ä¸Š8ç‚¹è¿è¡Œ
    - cron: '0 0 * * 1'
  workflow_dispatch:

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4

      - name: Run scraper
        run: python hr_news_scraper.py

      - name: Commit changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add hr_news_data.json
          git commit -m "Auto update: $(date +'%Y-%m-%d')" || echo "No changes"
          git push
```

### æ–¹æ¡ˆ2ï¼šWindowsä»»åŠ¡è®¡åˆ’

1. æ‰“å¼€"ä»»åŠ¡è®¡åˆ’ç¨‹åº"
2. åˆ›å»ºåŸºæœ¬ä»»åŠ¡
3. è§¦å‘å™¨ï¼šæ¯å‘¨ä¸€æ—©ä¸Š8ç‚¹
4. æ“ä½œï¼šå¯åŠ¨ç¨‹åº
   - ç¨‹åºï¼š`python`
   - å‚æ•°ï¼š`C:\Users\gaohanqi\hr_news_scraper.py`
5. å®Œæˆè®¾ç½®

## ğŸ›  é«˜çº§åŠŸèƒ½

### å®ç°çœŸå®çˆ¬è™«

åœ¨ `hr_news_scraper.py` ä¸­æ·»åŠ å…·ä½“ç½‘ç«™çš„çˆ¬å–é€»è¾‘ï¼š

```python
def scrape_hrbar(self):
    """æŠ“å–HR Baræ–°é—»"""
    url = "https://www.hrbar.com/news"
    response = requests.get(url, headers=self.headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # æ ¹æ®ç½‘ç«™ç»“æ„æå–ä¿¡æ¯
    articles = soup.find_all('article', class_='news-item')
    for article in articles:
        title = article.find('h2').text
        link = article.find('a')['href']
        # ... æå–å…¶ä»–ä¿¡æ¯

        if self.is_ai_related(title):
            category = self.categorize_content(title, '')
            self.news_data.append({
                'title': title,
                'link': link,
                'category': category,
                # ...
            })
```

### ä½¿ç”¨RSSè®¢é˜…

å®‰è£… feedparserï¼š
```bash
pip install feedparser
```

æ·»åŠ RSSè§£æä»£ç ï¼š
```python
import feedparser

def scrape_rss(self, rss_url):
    feed = feedparser.parse(rss_url)
    for entry in feed.entries:
        if self.is_ai_related(entry.title):
            # å¤„ç†æ¡ç›®...
```

### ä½¿ç”¨APIæ¥å£

å¸¸ç”¨çš„æ–°é—»APIï¼š
- NewsAPI: https://newsapi.org/
- èšåˆæ•°æ®: https://www.juhe.cn/
- å¤©èšæ•°æ®: https://www.tianapi.com/

## ğŸ“Š æ•°æ®æºæ¨è

### å›½å†…HRèµ„è®¯ç½‘ç«™
1. **HRoot** (https://www.hroot.com/)
2. **äººåŠ›èµ„æºç®¡ç†ç½‘** (https://www.hr.com.cn/)
3. **ä¸‰èŒ…äººåŠ›èµ„æºç½‘** (https://www.hrloo.com/)
4. **HRæ²™é¾™** (https://www.hrbar.com/)

### ç§‘æŠ€åª’ä½“HRé¢‘é“
1. **36æ°ª** - HRç§‘æŠ€é¢‘é“
2. **è™å—…** - å•†ä¸šç§‘æŠ€
3. **é’›åª’ä½“** - ä¼ä¸šæœåŠ¡

### å¾®ä¿¡å…¬ä¼—å·
1. HRTechChina
2. äººåŠ›èµ„æºæ™ºäº«ä¼š
3. HRç§‘æŠ€äº‘å›¾

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **çˆ¬è™«åˆè§„æ€§**
   - éµå®ˆç½‘ç«™ robots.txt
   - æ§åˆ¶è¯·æ±‚é¢‘ç‡
   - ä¸è¦ç”¨äºå•†ä¸šç”¨é€”

2. **åçˆ¬è™«åº”å¯¹**
   - ä½¿ç”¨ä»£ç†IPæ± 
   - æ·»åŠ éšæœºå»¶è¿Ÿ
   - æ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸º

3. **æ•°æ®è´¨é‡**
   - å®šæœŸæ£€æŸ¥æ•°æ®å‡†ç¡®æ€§
   - è¿‡æ»¤æ— æ•ˆé“¾æ¥
   - å»é‡å¤„ç†

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“ è®¸å¯

MIT License

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- GitHub Issues
- Email: your-email@example.com

---

**æœ€åæ›´æ–°ï¼š** 2026å¹´2æœˆ
**ç‰ˆæœ¬ï¼š** v1.0.0
